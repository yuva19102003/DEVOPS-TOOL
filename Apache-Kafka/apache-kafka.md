# üî∑ What is Apache Kafka?

**Kafka** is a **distributed event streaming platform** used to:

* Move data **reliably**
* Handle **high throughput**
* Process data **in real time**

![Image](https://daxg39y63pxwu.cloudfront.net/images/blog/apache-kafka-architecture-/image_589142173211625734253276.png)

![Image](https://miro.medium.com/1%2A-K1CkNCrENJXNrPg2PTETw.jpeg)

![Image](https://miro.medium.com/1%2AxnGngkxdXB9MImIFZwpT8Q.png)

![Image](https://dz2cdn1.dzone.com/storage/temp/14018525-kafka-architecture-topics-replication-to-partition-0.png)

### Kafka is used for:

* Event-driven microservices
* Log aggregation
* Metrics & monitoring
* Real-time analytics
* Stream processing (with Kafka Streams / Flink)

---

## üî∑ Kafka Core Concepts (Must Understand)

### 1Ô∏è‚É£ Events (Messages)

An **event** = key + value + timestamp
Example:

```json
{
  "user_id": "123",
  "action": "login"
}
```

---

### 2Ô∏è‚É£ Topics

A **topic** is a named stream of events.

Example:

```text
user-events
payment-events
logs
```

Topics are:

* Append-only
* Immutable
* Distributed

---

### 3Ô∏è‚É£ Partitions (üî• VERY IMPORTANT)

Each topic is split into **partitions**.

Why?

* Parallelism
* Scalability
* Ordering (per partition)

```
Topic: user-events
 ‚îú‚îÄ Partition 0
 ‚îú‚îÄ Partition 1
 ‚îî‚îÄ Partition 2
```

üëâ Ordering is guaranteed **only within a partition**, not across the topic.

---

### 4Ô∏è‚É£ Offsets

Each message has an **offset** (sequence number).

```
Partition 0:
offset 0 -> event A
offset 1 -> event B
offset 2 -> event C
```

Consumers track offsets ‚Üí enables **replay**.

---

### 5Ô∏è‚É£ Producers

* Send data to Kafka
* Choose topic + key
* Kafka decides partition

---

### 6Ô∏è‚É£ Consumers

* Read data from Kafka
* Belong to **consumer groups**

---

### 7Ô∏è‚É£ Consumer Groups (Critical)

* One partition ‚Üí consumed by **only one consumer in a group**
* Enables horizontal scaling

```
Topic: 3 partitions
Consumer Group:
 ‚îú‚îÄ Consumer 1 ‚Üí Partition 0
 ‚îú‚îÄ Consumer 2 ‚Üí Partition 1
 ‚îî‚îÄ Consumer 3 ‚Üí Partition 2
```

---

### 8Ô∏è‚É£ Brokers

Kafka runs as a **cluster**.

Each Kafka server = **Broker**

```
Broker 1
Broker 2
Broker 3
```

---

### 9Ô∏è‚É£ Replication (Fault Tolerance)

Each partition has:

* Leader
* Followers

If leader dies ‚Üí follower becomes leader.

---

## üî∑ Kafka Architecture (Big Picture)

![Image](https://elastic-stack.readthedocs.io/en/latest/_images/elk_kafka_arch.png)

![Image](https://developers.redhat.com/sites/default/files/RHOSAK%20LP1%20Fig4.png)

![Image](https://tomlee.co/img/KafkaRebalance.png)

```
Producers
   ‚Üì
Kafka Cluster (Brokers)
   ‚Üì
Consumers
```

---

## üî∑ Installation (Local Setup with Docker) ‚úÖ

### üì¶ Prerequisites

* Docker
* Docker Compose

---

### üîπ docker-compose.yml (Kafka + Zookeeper)

```yaml
version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

Start Kafka:

```bash
docker compose up -d
```

Check:

```bash
docker ps
```

---

## üî∑ Kafka CLI Hands-On (VERY IMPORTANT)

### 1Ô∏è‚É£ Create Topic

```bash
docker exec -it kafka kafka-topics \
--create \
--topic user-events \
--bootstrap-server localhost:9092 \
--partitions 3 \
--replication-factor 1
```

---

### 2Ô∏è‚É£ List Topics

```bash
docker exec -it kafka kafka-topics \
--list \
--bootstrap-server localhost:9092
```

---

### 3Ô∏è‚É£ Describe Topic

```bash
docker exec -it kafka kafka-topics \
--describe \
--topic user-events \
--bootstrap-server localhost:9092
```

---

### 4Ô∏è‚É£ Produce Messages

```bash
docker exec -it kafka kafka-console-producer \
--topic user-events \
--bootstrap-server localhost:9092
```

Type:

```text
user1 login
user2 logout
user3 signup
```

---

### 5Ô∏è‚É£ Consume Messages

```bash
docker exec -it kafka kafka-console-consumer \
--topic user-events \
--from-beginning \
--bootstrap-server localhost:9092
```

---

## üî∑ Consumer Groups (Hands-On)

### Start consumer with group

```bash
docker exec -it kafka kafka-console-consumer \
--topic user-events \
--group user-service \
--bootstrap-server localhost:9092
```

---

### Describe consumer group

```bash
docker exec -it kafka kafka-consumer-groups \
--describe \
--group user-service \
--bootstrap-server localhost:9092
```

---

## üî∑ Kafka with Programming (Go Example)

### Producer (Go)

```go
writer := kafka.NewWriter(kafka.WriterConfig{
    Brokers: []string{"localhost:9092"},
    Topic:   "user-events",
})

writer.WriteMessages(context.Background(),
    kafka.Message{
        Key:   []byte("user1"),
        Value: []byte("login"),
    },
)
```

---

### Consumer (Go)

```go
reader := kafka.NewReader(kafka.ReaderConfig{
    Brokers: []string{"localhost:9092"},
    Topic:   "user-events",
    GroupID: "user-service",
})

msg, _ := reader.ReadMessage(context.Background())
fmt.Println(string(msg.Value))
```

---

## üî∑ Kafka Delivery Semantics

| Type          | Meaning                       |
| ------------- | ----------------------------- |
| At most once  | Possible data loss            |
| At least once | Possible duplicates           |
| Exactly once  | No loss, no duplicates (hard) |

üëâ Kafka defaults to **at least once**

---

## üî∑ Kafka vs RabbitMQ (Quick Compare)

| Feature           | Kafka         | RabbitMQ    |
| ----------------- | ------------- | ----------- |
| Message retention | Time-based    | Queue-based |
| Replay messages   | ‚úÖ Yes         | ‚ùå No        |
| Throughput        | Very high     | Medium      |
| Ordering          | Per partition | Per queue   |
| Use case          | Streaming     | Task queues |

---

## üî∑ Kafka in Real Production (DevOps View)

### üîê Security

* TLS (SSL)
* SASL authentication
* ACLs (topic-level access)

### üìä Monitoring

* Prometheus + Grafana
* Kafka Exporter
* Lag monitoring

### üì¶ Storage

* Disk-based
* Log compaction
* Retention policies

---

## üî∑ Advanced Kafka Ecosystem

| Tool            | Purpose                       |
| --------------- | ----------------------------- |
| Kafka Streams   | Stream processing             |
| Kafka Connect   | Source ‚Üí Kafka / Kafka ‚Üí Sink |
| Schema Registry | Avro/Protobuf schemas         |
| Apache Flink    | Advanced streaming            |

---

## üî∑ Common Mistakes üö®

‚ùå Too many partitions
‚ùå Ignoring consumer lag
‚ùå No retention policy
‚ùå Using Kafka like a DB
‚ùå Not monitoring disk usage

---

## üî∑ Mental Model (Remember This)

> **Kafka is not a queue ‚Äî it is a distributed commit log.**

---

## üî∑ Learning Roadmap (Recommended)

1Ô∏è‚É£ Kafka basics & CLI
2Ô∏è‚É£ Producers / Consumers
3Ô∏è‚É£ Consumer groups
4Ô∏è‚É£ Partition strategy
5Ô∏è‚É£ Retention & compaction
6Ô∏è‚É£ Security
7Ô∏è‚É£ Monitoring
8Ô∏è‚É£ Kafka Streams

---

## ‚úÖ What You Can Build Now

* Event-driven microservices
* Audit logs
* Metrics pipeline
* Real-time notifications
* Streaming ETL

---
