# ğŸ§  Architecture (what youâ€™re building)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web UI  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Schedulerâ”‚ â”€â”€â”€â”€â”€â–¶ â”‚   Redis   â”‚  (broker)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Celery   â”‚â—€â”€â”€â”€â”€â”€â”€ â”‚  Worker   â”‚ (n workers)
â”‚ Executor â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚ (metadata DB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Directory structure

```bash
airflow-celery/
â”œâ”€â”€ dags/
â”œâ”€â”€ logs/
â”œâ”€â”€ plugins/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

---

## ğŸ“„ `.env` file

```env
AIRFLOW_UID=50000
AIRFLOW_GID=0

POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
```

---

## ğŸ³ `docker-compose.yml`

```yaml
version: "3.8"

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.3
  environment:
    &airflow-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
    restart: always

  redis:
    image: redis:7
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    restart: always

  airflow-init:
    <<: *airflow-common
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@example.com
      "
    restart: "no"

volumes:
  postgres-db-volume:
```

---

## ğŸš€ How to run (step-by-step)

### 1ï¸âƒ£ Set permissions (important on Linux)

```bash
mkdir -p dags logs plugins
sudo chown -R 50000:0 .
```

---

### 2ï¸âƒ£ Initialize Airflow

```bash
docker compose up airflow-init
```

You should see **DB migration + admin user creation**.

---

### 3ï¸âƒ£ Start all services

```bash
docker compose up -d
```

---

### 4ï¸âƒ£ Access Airflow UI

```
http://localhost:8080
```

**Login**

```
Username: admin
Password: admin
```

---

## ğŸ§ª Verify Celery + Redis is working

### Check running containers

```bash
docker compose ps
```

You should see:

* airflow-webserver
* airflow-scheduler
* airflow-worker
* redis
* postgres

---

### Check worker logs

```bash
docker compose logs airflow-worker -f
```

You should see:

```
Connected to redis://redis:6379/0
```

---

### Check Redis manually

```bash
docker exec -it airflow-celery-redis-1 redis-cli
PING
```

Expected:

```
PONG
```

---

## ğŸ§© Sample DAG to test Celery execution

Create `dags/test_celery.py`

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def hello():
    print("Hello from Celery worker!")

with DAG(
    dag_id="celery_test",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
):
    PythonOperator(
        task_id="hello_task",
        python_callable=hello
    )
```

Trigger the DAG from UI â†’ watch logs â†’ **worker executes task** âœ…

---

## âš™ï¸ Scale Celery workers (real power)

```bash
docker compose up -d --scale airflow-worker=3
```

Now you have **3 parallel workers** processing tasks.

---

## ğŸ”¥ Why this setup is production-grade

âœ” CeleryExecutor (distributed)
âœ” Redis as broker
âœ” Postgres as metadata + result backend
âœ” Horizontally scalable workers
âœ” Clean separation of concerns

---

## ğŸ§­ Next things to learn (recommended)

1. Flower (Celery monitoring)
2. Airflow Pools & Queues
3. Task retries & SLA
4. Remote logging (S3 / GCS)
5. KubernetesExecutor vs CeleryExecutor

---

